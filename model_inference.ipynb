{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class MissingValueImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, enable_mlflow_logging=False):\n",
    "        self.num_cols = None\n",
    "        self.cat_cols = None\n",
    "        self.num_means = None\n",
    "        self.enable_mlflow_logging = enable_mlflow_logging\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Check if X is a DataFrame\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.tolist()\n",
    "            self.cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "            self.num_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "            self.num_means = X[self.num_cols].mean()\n",
    "            # self.num_means = X[self.num_cols].mean()\n",
    "        else:\n",
    "            # print(\"not dataframe MVI\")\n",
    "            # For numpy arrays, assume all columns are numeric\n",
    "            self.feature_names_in_ = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            self.cat_cols = []\n",
    "            self.num_cols = self.feature_names_in_\n",
    "            # Create a pandas Series with feature names as index\n",
    "            self.num_means = pd.Series(np.nanmean(X, axis=0), index=self.num_cols)\n",
    "\n",
    "        if self.enable_mlflow_logging and hasattr(mlflow, 'log_dict'):\n",
    "            mlflow.log_dict(self.num_means.to_dict(), \"imputer/num_means.json\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert to DataFrame if it's a numpy array\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Apply imputation\n",
    "        if self.cat_cols:\n",
    "            X_copy[self.cat_cols] = X_copy[self.cat_cols].fillna('Unknown')\n",
    "        \n",
    "        for col in self.num_cols:\n",
    "            if col in X_copy.columns:\n",
    "                # X_copy[col] = X_copy[col].fillna(self.num_means.get(col, 0))\n",
    "                X_copy[col] = X_copy[col].fillna(0)\n",
    "        if self.enable_mlflow_logging and hasattr(mlflow, 'log_dict'):\n",
    "            nan_counts = X_copy.isna().sum()\n",
    "            mlflow.log_dict(nan_counts[nan_counts > 0].to_dict(), \n",
    "                           \"imputer/remaining_nans.json\")\n",
    "        # print(\"missing:\")\n",
    "        # print(X_copy.head)\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_)\n",
    "    \n",
    "    \n",
    "class RFEFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator, n_features_to_select=10, step=1, \n",
    "                 enable_mlflow_logging=False):\n",
    "        self.estimator = estimator\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "        self.step = step\n",
    "        self.enable_mlflow_logging = enable_mlflow_logging\n",
    "        self.support_ = None\n",
    "        self.ranking_ = None\n",
    "        self.selected_features_ = []\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        from sklearn.feature_selection import RFE\n",
    "        \n",
    "        # Save original feature names\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names_in_ = X.columns.tolist()\n",
    "            X_num = X.select_dtypes(include=[np.number])\n",
    "            feature_names = X_num.columns\n",
    "        else:\n",
    "            # print(\"not dataframe RFE\")\n",
    "            self.feature_names_in_ = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            X_num = X  # Assume all features are numeric if numpy array\n",
    "            feature_names = np.array(self.feature_names_in_)\n",
    "        \n",
    "        rfe = RFE(estimator=clone(self.estimator),\n",
    "                  n_features_to_select=self.n_features_to_select,\n",
    "                  step=self.step)\n",
    "        rfe.fit(X_num, y)\n",
    "\n",
    "        self.support_ = rfe.support_\n",
    "        self.ranking_ = rfe.ranking_\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.selected_features_ = feature_names[self.support_].tolist()\n",
    "        else:\n",
    "            # For numpy arrays, keep track of feature indices\n",
    "            self.selected_indices_ = np.where(self.support_)[0]\n",
    "            self.selected_features_ = [self.feature_names_in_[i] for i in self.selected_indices_]\n",
    "\n",
    "        if self.enable_mlflow_logging and hasattr(mlflow, 'log_dict'):\n",
    "            mlflow.log_dict({\n",
    "                \"selected_features\": self.selected_features_,\n",
    "                \"feature_ranking\": {str(name): int(rank) for name, rank in zip(feature_names, self.ranking_)},\n",
    "                \"n_features_to_select\": self.n_features_to_select,\n",
    "                \"step\": self.step\n",
    "            }, \"feature_selection/rfe.json\")\n",
    "        # print(\"selected features:\")\n",
    "        # for f in self.selected_features_:\n",
    "        #     print(f)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # For pandas DataFrame, select columns by name\n",
    "            # Make sure all selected features exist in the input\n",
    "            available_features = [f for f in self.selected_features_ if f in X.columns]\n",
    "            # print(\"RFE:\")\n",
    "            # print(X.head)\n",
    "            return X[available_features]\n",
    "        else:\n",
    "            # For numpy arrays, select columns by index\n",
    "            if hasattr(self, 'selected_indices_'):\n",
    "                return X[:, self.selected_indices_]\n",
    "            else:\n",
    "                # Fallback (shouldn't happen if fit was called first)\n",
    "                return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.selected_features_)\n",
    "    \n",
    "    \n",
    "class CorrelationFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.8, enable_mlflow_logging=False):\n",
    "        self.threshold = threshold\n",
    "        self.enable_mlflow_logging = enable_mlflow_logging\n",
    "        self.features_to_drop_ = []\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check if X is a DataFrame\n",
    "        is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        if is_dataframe:\n",
    "            # Store original feature names\n",
    "            self.feature_names_in_ = X.columns.tolist()\n",
    "            numeric_X = X.select_dtypes(include=[np.number]).copy()\n",
    "            numeric_X['target'] = y\n",
    "            feature_names = numeric_X.columns[:-1]  # Exclude 'target'\n",
    "        else:\n",
    "            # print(\"not dataframe\")\n",
    "            self.feature_names_in_ = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "            numeric_X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "            numeric_X['target'] = y\n",
    "            feature_names = numeric_X.columns[:-1]  # Exclude 'target'\n",
    "        \n",
    "        corr_matrix = numeric_X.corr().abs()\n",
    "\n",
    "        upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        upper_matrix = pd.DataFrame(upper, index=corr_matrix.index, \n",
    "                                  columns=corr_matrix.columns)\n",
    "\n",
    "        to_drop = []\n",
    "        for col in upper_matrix.columns:\n",
    "            high_corr = corr_matrix[col][upper_matrix[col]]\n",
    "            for row, value in high_corr.items():\n",
    "                if value > self.threshold:\n",
    "                    if corr_matrix[col]['target'] < corr_matrix[row]['target']:\n",
    "                        to_drop.append(col)\n",
    "                    else:\n",
    "                        to_drop.append(row)\n",
    "\n",
    "        # Filter out 'target' if it ended up in to_drop\n",
    "        to_drop = [col for col in to_drop if col != 'target']\n",
    "        self.features_to_drop_ = list(set(to_drop))\n",
    "        \n",
    "        if self.enable_mlflow_logging:\n",
    "            mlflow.log_dict({\n",
    "                \"features_dropped\": self.features_to_drop_,\n",
    "                \"correlation_threshold\": self.threshold\n",
    "            }, \"feature_selection/corr_filter.json\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # print(\"corr:\")\n",
    "            # print(X.drop(columns=self.features_to_drop_, errors='ignore').head())\n",
    "            return X.drop(columns=self.features_to_drop_, errors='ignore')\n",
    "        else:\n",
    "            # For numpy arrays, identify indices of features to keep\n",
    "            keep_indices = [i for i, name in enumerate(self.feature_names_in_) \n",
    "                           if name not in self.features_to_drop_]\n",
    "            return X[:, keep_indices]\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            input_features = self.feature_names_in_\n",
    "        return np.array([feat for feat in input_features if feat not in self.features_to_drop_])\n",
    "    \n",
    "class CustomPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_splits=5, smoothing=4, encoding_threshold=3, \n",
    "                 enable_mlflow_logging=False):\n",
    "        self.n_splits = n_splits\n",
    "        self.smoothing = smoothing\n",
    "        self.encoding_threshold = encoding_threshold\n",
    "        self.enable_mlflow_logging = enable_mlflow_logging\n",
    "        self.kfold_mappings = {}\n",
    "        self.global_means = {}\n",
    "        self.dummy_columns = []\n",
    "        self.cols_for_kfold = []\n",
    "        self.cols_for_onehot = []\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert X to DataFrame if it's a numpy array\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            # print(\"not dataframe\")\n",
    "            X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "        \n",
    "        self.feature_names_in_ = X.columns.tolist()\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        X_temp = X.copy()\n",
    "        X_temp['target'] = y\n",
    "\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Split columns based on unique value threshold\n",
    "        self.cols_for_kfold = []\n",
    "        self.cols_for_onehot = []\n",
    "        for col in categorical_cols:\n",
    "            if X[col].nunique() <= self.encoding_threshold:\n",
    "                self.cols_for_onehot.append(col)\n",
    "            else:\n",
    "                self.cols_for_kfold.append(col)\n",
    "\n",
    "        # Create k-fold encodings\n",
    "        for col in self.cols_for_kfold:\n",
    "            global_mean = X_temp['target'].mean()\n",
    "            self.global_means[col] = global_mean\n",
    "            self.kfold_mappings[col] = {}\n",
    "\n",
    "            for train_idx, val_idx in kf.split(X_temp):\n",
    "                train_fold = X_temp.iloc[train_idx]\n",
    "                category_means = train_fold.groupby(col)['target'].mean()\n",
    "                category_counts = train_fold.groupby(col)['target'].count()\n",
    "                smoothed_means = (\n",
    "                    category_means * category_counts + global_mean * self.smoothing\n",
    "                ) / (category_counts + self.smoothing)\n",
    "                self.kfold_mappings[col].update(smoothed_means.to_dict())\n",
    "\n",
    "        # Prepare transformed dataframe structure\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.cols_for_kfold:\n",
    "            X_transformed[f'{col}_encoded'] = X_transformed[col].map(\n",
    "                self.kfold_mappings[col]\n",
    "            ).fillna(self.global_means[col])\n",
    "            X_transformed.drop(columns=[col], inplace=True)\n",
    "\n",
    "        X_transformed = pd.get_dummies(\n",
    "            X_transformed,\n",
    "            columns=self.cols_for_onehot,\n",
    "            drop_first=True,\n",
    "            dummy_na=True,\n",
    "            dtype=int\n",
    "        )\n",
    "\n",
    "        self.dummy_columns = X_transformed.columns.tolist()\n",
    "        \n",
    "        if self.enable_mlflow_logging:\n",
    "            log_data = {\n",
    "                \"kfold_encoded\": self.cols_for_kfold,\n",
    "                \"one_hot_encoded\": self.cols_for_onehot,\n",
    "                \"final_features\": self.dummy_columns\n",
    "            }\n",
    "            mlflow.log_dict(log_data, \"encoding/features.json\")\n",
    "        # for k in self.cols_for_kfold:\n",
    "        #     print(k)\n",
    "        # for o in self.cols_for_onehot:\n",
    "        #     print(o)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert X to DataFrame if it's a numpy array\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            # print(\"not dataframe\")\n",
    "            X = pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "            \n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Apply k-fold encoding\n",
    "        for col in self.cols_for_kfold:\n",
    "            if col in X_transformed.columns:\n",
    "                X_transformed[f'{col}_encoded'] = X_transformed[col].map(\n",
    "                    self.kfold_mappings[col]\n",
    "                ).fillna(self.global_means.get(col, 0))\n",
    "                X_transformed.drop(columns=[col], inplace=True)\n",
    "\n",
    "        # Apply one-hot encoding\n",
    "        X_transformed = pd.get_dummies(\n",
    "            X_transformed,\n",
    "            columns=self.cols_for_onehot,\n",
    "            drop_first=True,\n",
    "            dummy_na=True,\n",
    "            dtype=int\n",
    "        )\n",
    "\n",
    "        # Ensure consistent columns with training data\n",
    "        missing_cols = set(self.dummy_columns) - set(X_transformed.columns)\n",
    "        for col in missing_cols:\n",
    "            X_transformed[col] = 0\n",
    "\n",
    "        extra_cols = set(X_transformed.columns) - set(self.dummy_columns)\n",
    "        if extra_cols:\n",
    "            X_transformed = X_transformed.drop(columns=list(extra_cols))\n",
    "            \n",
    "        # Ensure columns are in the same order\n",
    "        X_transformed = X_transformed[self.dummy_columns]\n",
    "        # print(\"prepro:\")\n",
    "        # print(X_transformed.head())\n",
    "        return X_transformed\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.dummy_columns)\n",
    "    \n"
   ],
   "id": "98e0b466360093b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "# Initialize DagsHub logging\n",
    "dagshub.init(repo_owner='arazm21', repo_name='ML-homework_1', mlflow=True)\n",
    "\n",
    "# Setup MLflow experiment\n",
    "experiment_name = \"cleaned_HW_1\"\n",
    "mlflow.set_experiment(experiment_name)"
   ],
   "id": "4e09ad4702d2886a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')\n",
    "# Load Data\n",
    "df = pd.read_csv('kaggle/input/train.csv')\n",
    "df_test = pd.read_csv('kaggle/input/test.csv')\n",
    "\n",
    "X = df.drop(columns=['SalePrice'])\n",
    "y = df['SalePrice']\n",
    "X_test = df_test\n",
    "train_ids = X.pop('Id')\n",
    "test_ids = X_test.pop('Id')\n",
    "\n",
    "print(X.shape, y.shape)"
   ],
   "id": "701b5c418b3e0106"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Apply best parameters and enable logging\n",
    "model = RandomForestRegressor(n_estimators=150, max_depth=6, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', MissingValueImputer(enable_mlflow_logging=True)),\n",
    "    ('preprocessor', CustomPreprocessor(enable_mlflow_logging=True, encoding_threshold=4, smoothing=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selector', CorrelationFeatureSelector(enable_mlflow_logging=True, threshold=0.85)),\n",
    "    ('rfe_selector', RFEFeatureSelector(estimator=model,enable_mlflow_logging=True, n_features_to_select=10, step=2)),\n",
    "    ('model', model)\n",
    "])\n",
    "run_name = \"inference\""
   ],
   "id": "1ce17844755a6b55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=f\"{run_name}_final_model\", nested=True):\n",
    "    # Clone and refit pipeline on full training data\n",
    "    final_pipeline = clone(pipeline)\n",
    "\n",
    "    final_pipeline.named_steps[\"imputer\"].fit(X)\n",
    "    X_full = final_pipeline.named_steps[\"imputer\"].transform(X)\n",
    "\n",
    "    final_pipeline.named_steps[\"preprocessor\"].fit(X_full, y)\n",
    "    X_full = final_pipeline.named_steps[\"preprocessor\"].transform(X_full)\n",
    "\n",
    "    final_pipeline.named_steps[\"scaler\"].fit(X_full)\n",
    "    X_full = final_pipeline.named_steps[\"scaler\"].transform(X_full)\n",
    "    X_full = pd.DataFrame(X_full, columns=final_pipeline.named_steps[\"preprocessor\"].get_feature_names_out())\n",
    "\n",
    "    final_pipeline.named_steps[\"feature_selector\"].fit(X_full, y)\n",
    "    X_full = final_pipeline.named_steps[\"feature_selector\"].transform(X_full)\n",
    "\n",
    "    final_pipeline.named_steps[\"rfe_selector\"].fit(X_full, y)\n",
    "    X_full = final_pipeline.named_steps[\"rfe_selector\"].transform(X_full)\n",
    "\n",
    "    # Train model\n",
    "    final_model = final_pipeline.named_steps[\"model\"]\n",
    "    final_model.fit(X_full, y)\n",
    "\n",
    "    # Prepare test data\n",
    "    X_test_trans = final_pipeline.named_steps[\"imputer\"].transform(X_test)\n",
    "    X_test_trans = final_pipeline.named_steps[\"preprocessor\"].transform(X_test_trans)\n",
    "    X_test_trans = final_pipeline.named_steps[\"scaler\"].transform(X_test_trans)\n",
    "    X_test_trans = pd.DataFrame(X_test_trans, columns=final_pipeline.named_steps[\"preprocessor\"].get_feature_names_out())\n",
    "    X_test_trans = final_pipeline.named_steps[\"feature_selector\"].transform(X_test_trans)\n",
    "    X_test_trans = final_pipeline.named_steps[\"rfe_selector\"].transform(X_test_trans)\n",
    "    mlflow.sklearn.log_model(model, artifact_path=\"final_model\")\n",
    "\n",
    "    # Predict\n",
    "    test_preds = final_model.predict(X_test_trans)\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": test_ids,          # Replace with your test set ID column\n",
    "        \"target\": test_preds     # Replace \"target\" with actual column name\n",
    "    })\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    mlflow.log_artifact(\"submission.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
